{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "from glob import glob"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 5.01592876e-01, -4.20712542e-01, -1.33449991e-01,\n         1.63951386e-01],\n       [-4.20712542e-01,  6.74275176e-01,  3.52944979e-01,\n        -3.03432845e-04],\n       [-1.33449991e-01,  3.52944979e-01,  8.46100168e-01,\n         4.70929056e-01],\n       [ 1.63951386e-01, -3.03432845e-04,  4.70929056e-01,\n         4.23714402e-01]])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "test = np.random.randn(3, 4, 5)\n",
    "np.cov(test[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "(3, 4, 10)"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(test, np.random.randn(5,10)).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8999999999999999\n"
     ]
    },
    {
     "data": {
      "text/plain": "2"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evals = np.array([4,3,2,1])\n",
    "evals = evals / np.sum(evals)\n",
    "contrib = np.cumsum(evals)\n",
    "print(contrib[2])\n",
    "sum(contrib > 0.85)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "POSE_FEATURE_PATH = \"D:/CU Files/IoT/Featurized_dataset/\"\n",
    "IMAGE_FEATURE_PATH = \"D:/CU Files/IoT/image_feature/resnet50/\"\n",
    "\n",
    "def get_path_dict(pose_feature_path,\n",
    "                  image_feature_path):\n",
    "    all_pose_path = [y for y in glob(os.path.join(pose_feature_path, \"*.mp4.npz\"))]\n",
    "    all_pose_name = [re.findall(\"Featurized_dataset\\\\\\\\(.+).mp4.npz\", path)[0]\n",
    "                     for path in all_pose_path]\n",
    "    all_image_path = [y for y in glob(os.path.join(image_feature_path, \"*.mp4.npz\"))]\n",
    "    all_image_name = [re.findall(image_feature_path[:-1] + \"\\\\\\\\(.+).mp4.npz\", path)[0] for path in all_image_path]\n",
    "    name_intersection = list(set(all_pose_name).intersection(set(all_image_name)))\n",
    "\n",
    "    print(\"{} pose feature files\".format(len(all_pose_name)))\n",
    "    print(\"{} image feature files\".format(len(all_image_name)))\n",
    "    print(\"{} feature files available\".format(len(name_intersection)))\n",
    "\n",
    "\n",
    "    pi_path_dict = {}   # get a dictionary which records the pose and image feature path\n",
    "    for i, feature_name in enumerate(all_pose_name):\n",
    "        try:\n",
    "            idx = all_image_name.index(feature_name)\n",
    "        except:\n",
    "            continue\n",
    "        pose_path = all_pose_path[i]\n",
    "        image_path = all_image_path[idx]\n",
    "        pi_path_dict[feature_name] = (pose_path, image_path)\n",
    "\n",
    "    return pi_path_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "977 pose feature files\n",
      "969 image feature files\n",
      "969 feature files available\n"
     ]
    }
   ],
   "source": [
    "path_dict = get_path_dict(pose_feature_path=POSE_FEATURE_PATH, image_feature_path=IMAGE_FEATURE_PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127, 34)\n",
      "(127, 2048)\n",
      "(127, 2082)\n"
     ]
    }
   ],
   "source": [
    "for feature_name, (pose_path, image_path) in path_dict.items():\n",
    "    pose_file = np.load(pose_path, allow_pickle=True)\n",
    "    trajectory = []\n",
    "    for i, (_, k) in enumerate(pose_file[\"keypoints\"]):\n",
    "        if len(k) != 0:\n",
    "            two_d_point = k[0, [0,1], :]\n",
    "            trajectory.append(two_d_point)\n",
    "    trajectory = np.stack(trajectory, axis=0).reshape(len(trajectory), -1)\n",
    "    print(trajectory.shape)\n",
    "\n",
    "    image_file = np.load(image_path, allow_pickle=True)\n",
    "    image_feature = image_file['feature']\n",
    "    print(image_feature.shape)\n",
    "    print(np.hstack((trajectory, image_feature)).shape)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class mydataset(data.Dataset):\n",
    "    def __init__(self, path_dict, pose=True, image=True):\n",
    "        self.path_dict = path_dict\n",
    "        self.Data, self.Label = self._get_features(pose, image)\n",
    "\n",
    "    def _get_features(self, pose=True, image=True):\n",
    "        features = []\n",
    "        labels = []\n",
    "        self.__label_encoder = {'no_interaction':0,\n",
    "                                'open_close_fridge':1,\n",
    "                                'put_back_item':2,\n",
    "                                'screen_interaction':3,\n",
    "                                'take_out_item':4}\n",
    "        for feature_name, (pose_path, image_path) in path_dict.items():\n",
    "            label = '_'.join(feature_name.split('_')[:-3])\n",
    "            labels.append(self.__label_encoder[label])\n",
    "            this_feature = self._get_single_feature(pose_path, image_path, pose, image)\n",
    "            features.append(this_feature)\n",
    "\n",
    "        return features, labels\n",
    "\n",
    "    def _get_single_feature(self, pose_path, image_path, pose, image):\n",
    "        if not pose:\n",
    "            image_file = np.load(image_path, allow_pickle=True)\n",
    "            image_feature = image_file[\"feature\"]\n",
    "            return image_feature\n",
    "        elif not image:\n",
    "            pose_file = np.load(pose_path, allow_pickle=True)\n",
    "            pose_feature = self._extract_trajectories(pose_file[\"keypoints\"])\n",
    "            return pose_feature\n",
    "        else:\n",
    "            image_file = np.load(image_path, allow_pickle=True)\n",
    "            image_feature = image_file[\"feature\"]\n",
    "            pose_file = np.load(pose_path, allow_pickle=True)\n",
    "            pose_feature = self._extract_trajectories(pose_file[\"keypoints\"])\n",
    "            assert image_feature.shape[0] == pose_feature.shape[0], \"number of frames mismatch\"\n",
    "            return np.hstack((pose_feature, image_feature))\n",
    "\n",
    "    def _extract_trajectories(self, keypoints):\n",
    "        trajectory = []\n",
    "        for i, (_, k) in enumerate(keypoints):\n",
    "            if len(k) != 0:\n",
    "                two_d_point = k[0, [0,1], :]\n",
    "                trajectory.append(two_d_point)\n",
    "        trajectory = np.stack(trajectory, axis=0).reshape(len(trajectory), -1)\n",
    "        return trajectory\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        seq = torch.from_numpy(self.Data[index])\n",
    "        label = torch.tensor(self.Label[index])\n",
    "        return seq, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Label)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    seq_list = [item[0] for item in batch]\n",
    "    labels = torch.LongTensor([item[1] for item in batch])\n",
    "    return seq_list, labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def data_split(path_dict, mode=\"tt\"):\n",
    "    if mode == \"tt\":\n",
    "        train_path_dict = {}\n",
    "        test_path_dict = {}\n",
    "        for key, value in path_dict.items():\n",
    "            u = np.random.uniform(0, 1)\n",
    "            if u < 0.9:\n",
    "                train_path_dict[key] = value\n",
    "            else:\n",
    "                test_path_dict[key] = value\n",
    "        return train_path_dict, test_path_dict\n",
    "    if mode == \"tvt\":\n",
    "        train_path_dict = {}\n",
    "        valid_path_dict = {}\n",
    "        test_path_dict = {}\n",
    "        for key, value in path_dict.items():\n",
    "            u = np.random.uniform(0, 1)\n",
    "            if u < 0.8:\n",
    "                train_path_dict[key] = value\n",
    "            elif 0.8 < u < 0.9:\n",
    "                valid_path_dict[key] = value\n",
    "            else:\n",
    "                test_path_dict[key] = value\n",
    "        return train_path_dict, valid_path_dict, test_path_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "train_path_dict, test_path_dict = data_split(path_dict, mode=\"tt\")\n",
    "train_dataset = mydataset(train_path_dict, pose=True, image=False)\n",
    "train_loader = data.DataLoader(train_dataset, collate_fn=collate_fn, batch_size=32, shuffle=True)\n",
    "test_dataset = mydataset(test_path_dict, pose=True, image=False)\n",
    "test_loader = data.DataLoader(test_dataset, collate_fn=collate_fn, batch_size=20, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([222, 34])\n"
     ]
    }
   ],
   "source": [
    "for X, Y in train_loader:\n",
    "    print(X[0].shape)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size=2082, hidden_size=1024):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=3,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 5)\n",
    "        )\n",
    "\n",
    "    def forward(self, sequences):\n",
    "        outputs1 = []\n",
    "        outputs2 = []\n",
    "        outputs3 = []\n",
    "        for seq in sequences:\n",
    "            out, (hidden, _) = self.lstm(seq)\n",
    "            outputs1.append(hidden[0,:])\n",
    "            outputs2.append(hidden[1,:])\n",
    "            outputs3.append(hidden[2,:])\n",
    "        outputs1 = torch.stack(outputs1)\n",
    "        outputs2 = torch.stack(outputs2)\n",
    "        outputs3 = torch.stack(outputs3)\n",
    "\n",
    "        y1 = self.fc(outputs1)\n",
    "        y2 = self.fc(outputs2)\n",
    "        y3 = self.fc(outputs3)\n",
    "\n",
    "        return y1, y2 ,y3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "584"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_list = list(map(lambda x: x.shape[0], train_dataset.Data))\n",
    "length_list.index(1093)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def get_acc(output, label):\n",
    "    total = output.shape[0]\n",
    "    _, pred_label = output.max(1)\n",
    "    num_correct = (pred_label == label).sum().item()\n",
    "    return num_correct / total"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def train(model, train_loader, valid_loader, epochs, optimizer, criterion, device=torch.device(\"cuda:0\")):\n",
    "    prev_time = time.time()\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        valid_loss = 0\n",
    "        valid_acc = 0\n",
    "        model.train()\n",
    "        for X, Y in train_loader:\n",
    "            # forward\n",
    "            ## move to device\n",
    "            X = [x.to(device) for x in X]\n",
    "            Y = Y.to(device)\n",
    "            out1, out2, out3 = model(X)\n",
    "            loss1 = criterion(out1, Y)\n",
    "            loss2 = criterion(out2, Y)\n",
    "            loss3 = criterion(out3, Y)\n",
    "            loss = 0.2 * loss1 + 0.3 * loss2 + 0.5 * loss3\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_acc += get_acc(out3, Y)\n",
    "\n",
    "        # evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for X, Y in valid_loader:\n",
    "                X = [x.to(device) for x in X]\n",
    "                Y = Y.to(device)\n",
    "                out1, out2, out3 = model(X)\n",
    "                loss = criterion(out3, Y)\n",
    "                valid_loss += loss.item()\n",
    "                valid_acc += get_acc(out3, Y)\n",
    "\n",
    "        print(\"Epoch {}   Train Loss:{:.3f}   Train Acc:{:.3f}   Valid Loss:{:.3f}   Valid Acc:{:.3f}   Time:{}\".format(\n",
    "            epoch, train_loss / len(train_loader), train_acc / len(train_loader), valid_loss / len(test_loader), valid_acc / len(test_loader), time.time() - prev_time\n",
    "        ))\n",
    "        prev_time = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0   Train Loss:1.638   Train Acc:0.220   Valid Loss:1.670   Valid Acc:0.209   Time:18.04733633995056\n",
      "Epoch 1   Train Loss:1.610   Train Acc:0.215   Valid Loss:1.661   Valid Acc:0.209   Time:13.971253395080566\n",
      "Epoch 2   Train Loss:1.604   Train Acc:0.217   Valid Loss:1.608   Valid Acc:0.214   Time:14.00832200050354\n",
      "Epoch 3   Train Loss:1.609   Train Acc:0.210   Valid Loss:1.605   Valid Acc:0.220   Time:14.0246000289917\n",
      "Epoch 4   Train Loss:1.607   Train Acc:0.221   Valid Loss:1.599   Valid Acc:0.248   Time:14.026754140853882\n",
      "Epoch 5   Train Loss:1.597   Train Acc:0.256   Valid Loss:1.635   Valid Acc:0.207   Time:14.023938417434692\n",
      "Epoch 6   Train Loss:1.599   Train Acc:0.220   Valid Loss:1.613   Valid Acc:0.212   Time:14.031240463256836\n",
      "Epoch 7   Train Loss:1.593   Train Acc:0.248   Valid Loss:1.660   Valid Acc:0.206   Time:14.01772665977478\n",
      "Epoch 8   Train Loss:1.594   Train Acc:0.239   Valid Loss:1.632   Valid Acc:0.230   Time:14.030089616775513\n",
      "Epoch 9   Train Loss:1.599   Train Acc:0.223   Valid Loss:1.590   Valid Acc:0.230   Time:14.028281688690186\n",
      "Epoch 10   Train Loss:1.590   Train Acc:0.244   Valid Loss:1.598   Valid Acc:0.250   Time:14.033967018127441\n",
      "Epoch 11   Train Loss:1.579   Train Acc:0.242   Valid Loss:1.584   Valid Acc:0.222   Time:14.027318716049194\n",
      "Epoch 12   Train Loss:1.583   Train Acc:0.232   Valid Loss:1.623   Valid Acc:0.235   Time:14.024835586547852\n",
      "Epoch 13   Train Loss:1.595   Train Acc:0.234   Valid Loss:1.654   Valid Acc:0.215   Time:14.028264045715332\n",
      "Epoch 14   Train Loss:1.595   Train Acc:0.233   Valid Loss:1.616   Valid Acc:0.217   Time:14.02959418296814\n",
      "Epoch 15   Train Loss:1.588   Train Acc:0.234   Valid Loss:1.650   Valid Acc:0.210   Time:14.031070709228516\n",
      "Epoch 16   Train Loss:1.587   Train Acc:0.247   Valid Loss:1.676   Valid Acc:0.214   Time:14.062809467315674\n",
      "Epoch 17   Train Loss:1.587   Train Acc:0.233   Valid Loss:1.609   Valid Acc:0.256   Time:14.053064823150635\n",
      "Epoch 18   Train Loss:1.592   Train Acc:0.244   Valid Loss:1.646   Valid Acc:0.205   Time:14.025787830352783\n",
      "Epoch 19   Train Loss:1.581   Train Acc:0.245   Valid Loss:1.586   Valid Acc:0.226   Time:14.073678731918335\n",
      "Epoch 20   Train Loss:1.569   Train Acc:0.240   Valid Loss:1.588   Valid Acc:0.216   Time:14.022254228591919\n",
      "Epoch 21   Train Loss:1.572   Train Acc:0.259   Valid Loss:1.662   Valid Acc:0.223   Time:14.018195152282715\n",
      "Epoch 22   Train Loss:1.574   Train Acc:0.261   Valid Loss:1.600   Valid Acc:0.268   Time:14.030604839324951\n",
      "Epoch 23   Train Loss:1.586   Train Acc:0.257   Valid Loss:1.598   Valid Acc:0.249   Time:14.01852822303772\n",
      "Epoch 24   Train Loss:1.583   Train Acc:0.262   Valid Loss:1.609   Valid Acc:0.238   Time:14.02596402168274\n",
      "Epoch 25   Train Loss:1.564   Train Acc:0.280   Valid Loss:1.594   Valid Acc:0.245   Time:14.026498556137085\n",
      "Epoch 26   Train Loss:1.566   Train Acc:0.264   Valid Loss:1.661   Valid Acc:0.217   Time:14.038349628448486\n",
      "Epoch 27   Train Loss:1.563   Train Acc:0.261   Valid Loss:1.563   Valid Acc:0.254   Time:14.368573188781738\n",
      "Epoch 28   Train Loss:1.561   Train Acc:0.273   Valid Loss:1.653   Valid Acc:0.233   Time:14.380723714828491\n",
      "Epoch 29   Train Loss:1.559   Train Acc:0.278   Valid Loss:1.592   Valid Acc:0.250   Time:14.332388401031494\n",
      "Epoch 30   Train Loss:1.537   Train Acc:0.310   Valid Loss:1.665   Valid Acc:0.227   Time:14.317621231079102\n",
      "Epoch 31   Train Loss:1.546   Train Acc:0.314   Valid Loss:1.723   Valid Acc:0.201   Time:14.372028112411499\n",
      "Epoch 32   Train Loss:1.520   Train Acc:0.320   Valid Loss:1.754   Valid Acc:0.242   Time:14.514982461929321\n",
      "Epoch 33   Train Loss:1.499   Train Acc:0.346   Valid Loss:1.750   Valid Acc:0.230   Time:14.251973390579224\n",
      "Epoch 34   Train Loss:1.491   Train Acc:0.364   Valid Loss:1.703   Valid Acc:0.262   Time:14.283167839050293\n",
      "Epoch 35   Train Loss:1.493   Train Acc:0.334   Valid Loss:2.109   Valid Acc:0.235   Time:14.22647500038147\n",
      "Epoch 36   Train Loss:1.487   Train Acc:0.337   Valid Loss:2.189   Valid Acc:0.204   Time:14.452279806137085\n",
      "Epoch 37   Train Loss:1.474   Train Acc:0.373   Valid Loss:2.174   Valid Acc:0.204   Time:14.122709512710571\n",
      "Epoch 38   Train Loss:1.481   Train Acc:0.361   Valid Loss:2.113   Valid Acc:0.209   Time:14.075998067855835\n",
      "Epoch 39   Train Loss:1.462   Train Acc:0.373   Valid Loss:2.635   Valid Acc:0.202   Time:14.199581384658813\n",
      "Epoch 40   Train Loss:1.459   Train Acc:0.369   Valid Loss:2.191   Valid Acc:0.211   Time:14.057555913925171\n",
      "Epoch 41   Train Loss:1.445   Train Acc:0.371   Valid Loss:2.240   Valid Acc:0.201   Time:14.215242862701416\n",
      "Epoch 42   Train Loss:1.465   Train Acc:0.387   Valid Loss:2.944   Valid Acc:0.197   Time:14.137417554855347\n",
      "Epoch 43   Train Loss:1.467   Train Acc:0.349   Valid Loss:2.590   Valid Acc:0.201   Time:14.270509719848633\n",
      "Epoch 44   Train Loss:1.446   Train Acc:0.385   Valid Loss:2.018   Valid Acc:0.223   Time:14.129260540008545\n",
      "Epoch 45   Train Loss:1.461   Train Acc:0.368   Valid Loss:2.335   Valid Acc:0.202   Time:13.977928638458252\n",
      "Epoch 46   Train Loss:1.451   Train Acc:0.377   Valid Loss:2.038   Valid Acc:0.228   Time:13.976789712905884\n",
      "Epoch 47   Train Loss:1.444   Train Acc:0.388   Valid Loss:2.295   Valid Acc:0.205   Time:14.006997108459473\n",
      "Epoch 48   Train Loss:1.443   Train Acc:0.384   Valid Loss:2.056   Valid Acc:0.218   Time:14.000025510787964\n",
      "Epoch 49   Train Loss:1.443   Train Acc:0.373   Valid Loss:2.288   Valid Acc:0.209   Time:14.003806829452515\n"
     ]
    }
   ],
   "source": [
    "lstm = LSTMClassifier(input_size=34, hidden_size=128)\n",
    "EPOCHS = 50\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=0.001)\n",
    "\n",
    "train(lstm, train_loader, test_loader, EPOCHS, optimizer, criterion)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "conda-env-gputest-py",
   "language": "python",
   "display_name": "Python [conda env:gputest]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}