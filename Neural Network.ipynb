{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "from glob import glob"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "POSE_FEATURE_PATH = \"D:/CU Files/IoT/Featurized_dataset/\"\n",
    "IMAGE_FEATURE_PATH = \"D:/CU Files/IoT/image_feature/resnet50/\"\n",
    "\n",
    "def get_path_dict(pose_feature_path,\n",
    "                  image_feature_path):\n",
    "    all_pose_path = [y for y in glob(os.path.join(pose_feature_path, \"*.mp4.npz\"))]\n",
    "    all_pose_name = [re.findall(\"Featurized_dataset\\\\\\\\(.+).mp4.npz\", path)[0]\n",
    "                     for path in all_pose_path]\n",
    "    all_image_path = [y for y in glob(os.path.join(image_feature_path, \"*.mp4.npz\"))]\n",
    "    all_image_name = [re.findall(image_feature_path[:-1] + \"\\\\\\\\(.+).mp4.npz\", path)[0] for path in all_image_path]\n",
    "    name_intersection = list(set(all_pose_name).intersection(set(all_image_name)))\n",
    "\n",
    "    print(\"{} pose feature files\".format(len(all_pose_name)))\n",
    "    print(\"{} image feature files\".format(len(all_image_name)))\n",
    "    print(\"{} feature files available\".format(len(name_intersection)))\n",
    "\n",
    "\n",
    "    pi_path_dict = {}   # get a dictionary which records the pose and image feature path\n",
    "    for i, feature_name in enumerate(all_pose_name):\n",
    "        try:\n",
    "            idx = all_image_name.index(feature_name)\n",
    "        except:\n",
    "            continue\n",
    "        pose_path = all_pose_path[i]\n",
    "        image_path = all_image_path[idx]\n",
    "        pi_path_dict[feature_name] = (pose_path, image_path)\n",
    "\n",
    "    return pi_path_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "977 pose feature files\n",
      "969 image feature files\n",
      "969 feature files available\n"
     ]
    }
   ],
   "source": [
    "path_dict = get_path_dict(pose_feature_path=POSE_FEATURE_PATH, image_feature_path=IMAGE_FEATURE_PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127, 34)\n",
      "(127, 2048)\n",
      "(127, 2082)\n"
     ]
    }
   ],
   "source": [
    "for feature_name, (pose_path, image_path) in path_dict.items():\n",
    "    pose_file = np.load(pose_path, allow_pickle=True)\n",
    "    trajectory = []\n",
    "    for i, (_, k) in enumerate(pose_file[\"keypoints\"]):\n",
    "        if len(k) != 0:\n",
    "            two_d_point = k[0, [0,1], :]\n",
    "            trajectory.append(two_d_point)\n",
    "    trajectory = np.stack(trajectory, axis=0).reshape(len(trajectory), -1)\n",
    "    print(trajectory.shape)\n",
    "\n",
    "    image_file = np.load(image_path, allow_pickle=True)\n",
    "    image_feature = image_file['feature']\n",
    "    print(image_feature.shape)\n",
    "    print(np.hstack((trajectory, image_feature)).shape)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [],
   "source": [
    "class mydataset(data.Dataset):\n",
    "    def __init__(self, path_dict):\n",
    "        self.path_dict = path_dict\n",
    "        self.Data, self.Label = self._get_features()\n",
    "\n",
    "    def _get_features(self, pose=True, image=True):\n",
    "        features = []\n",
    "        labels = []\n",
    "        self.__label_encoder = {'no_interaction':0,\n",
    "                                'open_close_fridge':1,\n",
    "                                'put_back_item':2,\n",
    "                                'screen_interaction':3,\n",
    "                                'take_out_item':4}\n",
    "        for feature_name, (pose_path, image_path) in path_dict.items():\n",
    "            label = '_'.join(feature_name.split('_')[:-3])\n",
    "            labels.append(self.__label_encoder[label])\n",
    "            this_feature = self._get_single_feature(pose_path, image_path, pose, image)\n",
    "            features.append(this_feature)\n",
    "\n",
    "        return features, labels\n",
    "\n",
    "    def _get_single_feature(self, pose_path, image_path, pose, image):\n",
    "        if pose is None:\n",
    "            image_file = np.load(image_path, allow_pickle=True)\n",
    "            image_feature = image_file[\"feature\"]\n",
    "            return image_feature\n",
    "        elif image is None:\n",
    "            pose_file = np.load(pose_path, allow_pickle=True)\n",
    "            pose_feature = self._extract_trajectories(pose_file[\"keypoints\"])\n",
    "            return pose_feature\n",
    "        else:\n",
    "            image_file = np.load(image_path, allow_pickle=True)\n",
    "            image_feature = image_file[\"feature\"]\n",
    "            pose_file = np.load(pose_path, allow_pickle=True)\n",
    "            pose_feature = self._extract_trajectories(pose_file[\"keypoints\"])\n",
    "            assert image_feature.shape[0] == pose_feature.shape[0], \"number of frames mismatch\"\n",
    "            return np.hstack((pose_feature, image_feature))\n",
    "\n",
    "    def _extract_trajectories(self, keypoints):\n",
    "        trajectory = []\n",
    "        for i, (_, k) in enumerate(keypoints):\n",
    "            if len(k) != 0:\n",
    "                two_d_point = k[0, [0,1], :]\n",
    "                trajectory.append(two_d_point)\n",
    "        trajectory = np.stack(trajectory, axis=0).reshape(len(trajectory), -1)\n",
    "        return trajectory\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        seq = torch.from_numpy(self.Data[index])\n",
    "        label = torch.tensor(self.Label[index])\n",
    "        return seq, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Label)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    seq_list = [item[0] for item in batch]\n",
    "    labels = torch.LongTensor([item[1] for item in batch])\n",
    "    return seq_list, labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [],
   "source": [
    "def data_split(path_dict, mode=\"tt\"):\n",
    "    if mode == \"tt\":\n",
    "        train_path_dict = {}\n",
    "        test_path_dict = {}\n",
    "        for key, value in path_dict.items():\n",
    "            u = np.random.uniform(0, 1)\n",
    "            if u < 0.9:\n",
    "                train_path_dict[key] = value\n",
    "            else:\n",
    "                test_path_dict[key] = value\n",
    "        return train_path_dict, test_path_dict\n",
    "    if mode == \"tvt\":\n",
    "        train_path_dict = {}\n",
    "        valid_path_dict = {}\n",
    "        test_path_dict = {}\n",
    "        for key, value in path_dict.items():\n",
    "            u = np.random.uniform(0, 1)\n",
    "            if u < 0.8:\n",
    "                train_path_dict[key] = value\n",
    "            elif 0.8 < u < 0.9:\n",
    "                valid_path_dict[key] = value\n",
    "            else:\n",
    "                test_path_dict[key] = value\n",
    "        return train_path_dict, valid_path_dict, test_path_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [],
   "source": [
    "train_path_dict, test_path_dict = data_split(path_dict, mode=\"tt\")\n",
    "train_dataset = mydataset(train_path_dict)\n",
    "train_loader = data.DataLoader(train_dataset, collate_fn=collate_fn, batch_size=32, shuffle=True)\n",
    "test_dataset = mydataset(test_path_dict)\n",
    "test_loader = data.DataLoader(test_dataset, collate_fn=collate_fn, batch_size=20, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size=2082, hidden_size=1024):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=3,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 5)\n",
    "        )\n",
    "\n",
    "    def forward(self, sequences):\n",
    "        outputs1 = []\n",
    "        outputs2 = []\n",
    "        outputs3 = []\n",
    "        for seq in sequences:\n",
    "            out, (hidden, _) = self.lstm(seq)\n",
    "            outputs1.append(hidden[0,:])\n",
    "            outputs2.append(hidden[1,:])\n",
    "            outputs3.append(hidden[2,:])\n",
    "        outputs1 = torch.stack(outputs1)\n",
    "        outputs2 = torch.stack(outputs2)\n",
    "        outputs3 = torch.stack(outputs3)\n",
    "\n",
    "        y1 = self.fc(outputs1)\n",
    "        y2 = self.fc(outputs2)\n",
    "        y3 = self.fc(outputs3)\n",
    "\n",
    "        return y1, y2 ,y3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [],
   "source": [
    "def get_acc(output, label):\n",
    "    total = output.shape[0]\n",
    "    _, pred_label = output.max(1)\n",
    "    num_correct = (pred_label == label).sum().item()\n",
    "    return num_correct / total"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "outputs": [],
   "source": [
    "def train(model, train_loader, valid_loader, epochs, optimizer, criterion):\n",
    "    prev_time = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        valid_loss = 0\n",
    "        valid_acc = 0\n",
    "        model = model.train()\n",
    "        for X, Y in train_loader:\n",
    "            model.train()\n",
    "            # forward\n",
    "            out1, out2, out3 = model(X)\n",
    "            loss1 = criterion(out1, Y)\n",
    "            loss2 = criterion(out2, Y)\n",
    "            loss3 = criterion(out3, Y)\n",
    "            loss = 0.2 * loss1 + 0.3 * loss2 + 0.5 * loss3\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_acc += get_acc(out3, Y)\n",
    "\n",
    "        # evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for X, Y in valid_loader:\n",
    "                out1, out2, out3 = model(X)\n",
    "                loss = criterion(out3, Y)\n",
    "                valid_loss += loss.item()\n",
    "                valid_acc += get_acc(out3, Y)\n",
    "\n",
    "        print(\"Epoch {}   Train Loss:{:.3f}   Train Acc:{:.3f}   Valid Loss:{:.3f}   Valid Acc:{:.3f}   Time:{}\".format(\n",
    "            epoch, train_loss / len(train_loader), train_acc / len(train_loader), valid_loss / len(test_loader), valid_acc / len(test_loader), time.time() - prev_time\n",
    "        ))\n",
    "        prev_time = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0   Train Loss:1.650   Train Acc:0.232   Valid Loss:1.724   Valid Acc:0.206   Time:6149.237930774689\n",
      "Epoch 1   Train Loss:1.608   Train Acc:0.253   Valid Loss:1.718   Valid Acc:0.210   Time:5671.530113697052\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-186-22c71c4e578c>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0moptimizer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptim\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mAdam\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlstm\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparameters\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlr\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0.0001\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m \u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlstm\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_loader\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest_loader\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mEPOCHS\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m<ipython-input-185-3b816248240b>\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(model, train_loader, valid_loader, epochs, optimizer, criterion)\u001B[0m\n\u001B[0;32m     18\u001B[0m             \u001B[1;31m# backward\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     19\u001B[0m             \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 20\u001B[1;33m             \u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     21\u001B[0m             \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    361\u001B[0m                 \u001B[0mcreate_graph\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    362\u001B[0m                 inputs=inputs)\n\u001B[1;32m--> 363\u001B[1;33m         \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    364\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    365\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    171\u001B[0m     \u001B[1;31m# some Python versions print out the first line of a multi-line function\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    172\u001B[0m     \u001B[1;31m# calls in the traceback and some print out the last line\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 173\u001B[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001B[0m\u001B[0;32m    174\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    175\u001B[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "lstm = LSTMClassifier()\n",
    "EPOCHS = 5\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=0.0001)\n",
    "\n",
    "train(lstm, train_loader, test_loader, EPOCHS, optimizer, criterion)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}